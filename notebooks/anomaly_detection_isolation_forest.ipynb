{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Anomaly Detection with Isolation Forest\n",
                "\n",
                "Using IsolationForest to detect early hospital readmissions (<30 days) as anomalies."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
                "from sklearn.metrics import precision_score, recall_score, classification_report\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 5)\n",
                "\n",
                "print(\"Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load preprocessed data\n",
                "# Assuming you have X and y from preprocessing\n",
                "# If loading from CSV:\n",
                "X = pd.read_csv('../data/processed/X_features.csv')\n",
                "y = pd.read_csv('../data/processed/y_target.csv')['target'].values\n",
                "\n",
                "print(f\"Feature matrix shape: {X.shape}\")\n",
                "print(f\"Target vector shape: {y.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(f\"  y=0 (normal, not readmitted <30): {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
                "print(f\"  y=1 (anomaly, readmitted <30):    {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Train/Test Split (Stratified)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified split to preserve class distribution\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    random_state=42, \n",
                "    stratify=y\n",
                ")\n",
                "\n",
                "print(\"Train/Test Split Complete\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Train set: {X_train.shape[0]:,} samples\")\n",
                "print(f\"  y=0: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
                "print(f\"  y=1: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\nTest set: {X_test.shape[0]:,} samples\")\n",
                "print(f\"  y=0: {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.1f}%)\")\n",
                "print(f\"  y=1: {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train Isolation Forest (on Normal Class Only)\n",
                "\n",
                "IsolationForest is an unsupervised method. We train it on the **majority class (y=0)** only, treating these as \"normal\" samples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get only normal samples from training set (y=0)\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "\n",
                "print(f\"Training IsolationForest on {len(X_train_normal):,} normal samples...\")\n",
                "\n",
                "# Initialize and fit IsolationForest\n",
                "# contamination: expected proportion of outliers (set to match actual anomaly rate)\n",
                "iso_forest = IsolationForest(\n",
                "    n_estimators=100,\n",
                "    max_samples='auto',\n",
                "    contamination=0.11,  # Approximately 11% anomalies based on data\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "# Fit on normal samples only\n",
                "iso_forest.fit(X_train_normal)\n",
                "\n",
                "print(\"✓ IsolationForest trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Compute Anomaly Scores\n",
                "\n",
                "IsolationForest returns:\n",
                "- **decision_function**: Higher (less negative) = more normal, Lower (more negative) = more anomalous\n",
                "- We'll convert to anomaly scores where **higher = more anomalous**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get anomaly scores (decision function)\n",
                "# Note: decision_function returns negative values where lower = more anomalous\n",
                "# We'll negate it so higher scores = more anomalous\n",
                "train_scores = -iso_forest.decision_function(X_train)\n",
                "test_scores = -iso_forest.decision_function(X_test)\n",
                "\n",
                "# Also get binary predictions (-1 for anomaly, 1 for normal)\n",
                "train_predictions = iso_forest.predict(X_train)\n",
                "test_predictions = iso_forest.predict(X_test)\n",
                "\n",
                "print(\"Anomaly Scores Computed\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Train scores - Min: {train_scores.min():.3f}, Max: {train_scores.max():.3f}, Mean: {train_scores.mean():.3f}\")\n",
                "print(f\"Test scores  - Min: {test_scores.min():.3f}, Max: {test_scores.max():.3f}, Mean: {test_scores.mean():.3f}\")\n",
                "\n",
                "# Predicted anomalies (default threshold)\n",
                "train_anomalies = (train_predictions == -1).sum()\n",
                "test_anomalies = (test_predictions == -1).sum()\n",
                "print(f\"\\nPredicted anomalies (default threshold):\")\n",
                "print(f\"  Train: {train_anomalies:,} ({train_anomalies/len(y_train)*100:.1f}%)\")\n",
                "print(f\"  Test:  {test_anomalies:,} ({test_anomalies/len(y_test)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation Metrics on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC-AUC\n",
                "roc_auc = roc_auc_score(y_test, test_scores)\n",
                "\n",
                "# Calculate Precision-Recall AUC\n",
                "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, test_scores)\n",
                "pr_auc = auc(recall_vals, precision_vals)\n",
                "\n",
                "print(\"Overall Metrics on Test Set\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"ROC-AUC:             {roc_auc:.4f}\")\n",
                "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
                "\n",
                "# Evaluate at different thresholds\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"Performance at Different Thresholds\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Define some percentile-based thresholds\n",
                "thresholds_to_test = [\n",
                "    np.percentile(test_scores, 90),  # Top 10% most anomalous\n",
                "    np.percentile(test_scores, 85),  # Top 15% most anomalous\n",
                "    np.percentile(test_scores, 80),  # Top 20% most anomalous\n",
                "]\n",
                "\n",
                "threshold_results = []\n",
                "\n",
                "for threshold in thresholds_to_test:\n",
                "    # Predict anomalies based on threshold\n",
                "    y_pred = (test_scores >= threshold).astype(int)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
                "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
                "    flagged_pct = (y_pred == 1).sum() / len(y_pred) * 100\n",
                "    \n",
                "    threshold_results.append({\n",
                "        'Threshold': threshold,\n",
                "        'Precision': precision,\n",
                "        'Recall': recall,\n",
                "        'Flagged (%)': flagged_pct\n",
                "    })\n",
                "    \n",
                "    print(f\"\\nThreshold: {threshold:.3f}\")\n",
                "    print(f\"  Precision: {precision:.3f}\")\n",
                "    print(f\"  Recall:    {recall:.3f}\")\n",
                "    print(f\"  Flagged:   {flagged_pct:.1f}% of test samples\")\n",
                "\n",
                "# Create summary DataFrame\n",
                "threshold_df = pd.DataFrame(threshold_results)\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"Summary Table\")\n",
                "print(\"=\" * 50)\n",
                "print(threshold_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC curve\n",
                "fpr, tpr, roc_thresholds = roc_curve(y_test, test_scores)\n",
                "\n",
                "# Create figure with subplots\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ========== ROC Curve ==========\n",
                "axes[0].plot(fpr, tpr, linewidth=2, label=f'IsolationForest (AUC = {roc_auc:.3f})')\n",
                "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
                "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
                "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
                "axes[0].set_title('ROC Curve - Anomaly Detection', fontsize=12, fontweight='bold')\n",
                "axes[0].legend(loc='lower right')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# ========== Precision-Recall Curve ==========\n",
                "axes[1].plot(recall_vals, precision_vals, linewidth=2, \n",
                "             label=f'IsolationForest (AUC = {pr_auc:.3f})')\n",
                "\n",
                "# Baseline (random classifier for imbalanced data)\n",
                "baseline_precision = (y_test == 1).sum() / len(y_test)\n",
                "axes[1].axhline(y=baseline_precision, color='k', linestyle='--', linewidth=1, \n",
                "                label=f'Baseline (Random) = {baseline_precision:.3f}')\n",
                "\n",
                "axes[1].set_xlabel('Recall', fontsize=11)\n",
                "axes[1].set_ylabel('Precision', fontsize=11)\n",
                "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
                "axes[1].legend(loc='best')\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "\n",
                "# Save figure\n",
                "from pathlib import Path\n",
                "output_dir = Path('../results/figures')\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "plt.savefig(output_dir / 'isolation_forest_evaluation.png', dpi=150, bbox_inches='tight')\n",
                "print(f\"✓ Saved plot to: {output_dir / 'isolation_forest_evaluation.png'}\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Additional Analysis: Score Distribution by Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize anomaly score distributions for both classes\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Histogram\n",
                "axes[0].hist(test_scores[y_test == 0], bins=50, alpha=0.6, label='Normal (y=0)', color='green')\n",
                "axes[0].hist(test_scores[y_test == 1], bins=50, alpha=0.6, label='Anomaly (y=1)', color='red')\n",
                "axes[0].set_xlabel('Anomaly Score', fontsize=11)\n",
                "axes[0].set_ylabel('Frequency', fontsize=11)\n",
                "axes[0].set_title('Distribution of Anomaly Scores by True Label', fontsize=12, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Box plot\n",
                "axes[1].boxplot([test_scores[y_test == 0], test_scores[y_test == 1]], \n",
                "                labels=['Normal (y=0)', 'Anomaly (y=1)'],\n",
                "                patch_artist=True,\n",
                "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
                "axes[1].set_ylabel('Anomaly Score', fontsize=11)\n",
                "axes[1].set_title('Anomaly Score Distribution (Box Plot)', fontsize=12, fontweight='bold')\n",
                "axes[1].grid(alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(output_dir / 'anomaly_score_distributions.png', dpi=150, bbox_inches='tight')\n",
                "print(f\"✓ Saved plot to: {output_dir / 'anomaly_score_distributions.png'}\")\n",
                "plt.show()\n",
                "\n",
                "# Statistical comparison\n",
                "print(\"\\nAnomaly Score Statistics by Class (Test Set)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Normal (y=0):  Mean = {test_scores[y_test==0].mean():.3f}, Std = {test_scores[y_test==0].std():.3f}\")\n",
                "print(f\"Anomaly (y=1): Mean = {test_scores[y_test==1].mean():.3f}, Std = {test_scores[y_test==1].std():.3f}\")\n",
                "print(f\"\\nMean difference: {test_scores[y_test==1].mean() - test_scores[y_test==0].mean():.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Takeaways:\n",
                "\n",
                "1. **ROC-AUC** measures overall discriminative ability (higher is better, 0.5 = random)\n",
                "2. **PR-AUC** is more informative for imbalanced datasets (our case: 89% normal, 11% anomalies)\n",
                "3. **Threshold selection** depends on operational requirements:\n",
                "   - Higher threshold → Higher precision, lower recall (fewer false alarms, miss more anomalies)\n",
                "   - Lower threshold → Lower precision, higher recall (catch more anomalies, more false alarms)\n",
                "\n",
                "### Performance Interpretation:\n",
                "- **ROC-AUC > 0.7**: Good separation between classes\n",
                "- **PR-AUC**: Compare to baseline (random classifier = 0.11 for this dataset)\n",
                "- **Threshold tuning**: Choose based on cost of false positives vs. false negatives in your use case\n",
                "\n",
                "### Next Steps:\n",
                "1. Compare with other anomaly detection methods (LOF, One-Class SVM)\n",
                "2. Try ontology-based rule validation on detected anomalies\n",
                "3. Feature importance analysis to understand what drives anomalies\n",
                "4. Ensemble multiple anomaly detection methods"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}