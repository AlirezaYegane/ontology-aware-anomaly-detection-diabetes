{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04. Ontology-Inspired Evaluation\n",
                "\n",
                "Combine ML anomaly scores with domain-knowledge-based ontology penalties and compare performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === UNIVERSAL PATH SETUP (Works in both Local and Colab) ===\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Auto-detect environment and setup paths\n",
                "try:\n",
                "    from src.utils import setup_paths\n",
                "    env_type = setup_paths()\n",
                "except ImportError:\n",
                "    # Fallback if utils not found (first run)\n",
                "    print(\"\u2699\ufe0f  Setting up paths...\")\n",
                "    try:\n",
                "        import google.colab\n",
                "        in_colab = True\n",
                "        if 'notebooks' in os.getcwd():\n",
                "            os.chdir('..')\n",
                "        project_root = os.getcwd()\n",
                "        print(\"\u2601\ufe0f  Detected: Google Colab\")\n",
                "    except ImportError:\n",
                "        in_colab = False\n",
                "        project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "        print(\"\ud83d\udcbb Detected: Local Environment\")\n",
                "    \n",
                "    if project_root not in sys.path:\n",
                "        sys.path.insert(0, project_root)\n",
                "    print(f\"\u2705 Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "from src.preprocessing import build_feature_matrix, train_test_split_stratified, get_selected_features, clean_data, create_target\n",
                "from src.models import fit_isolation_forest, score_isolation_forest\n",
                "from src.ontology import apply_ontology_rules, combine_scores\n",
                "from src.evaluation import evaluate_anomaly_detector, plot_roc_pr, save_metrics_summary, print_comparison_table\n",
                "\n",
                "# Create results directory\n",
                "results_dir = Path('../results')\n",
                "results_dir.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Raw Data (with original features for ontology rules)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We need the original data to apply ontology rules\n",
                "from src.preprocessing import load_raw_data\n",
                "\n",
                "data_path = '../data/raw/diabetic_data.csv'\n",
                "df_raw = load_raw_data(data_path)\n",
                "\n",
                "# Clean and prepare\n",
                "selected_features = get_selected_features()\n",
                "df_clean = clean_data(df_raw, selected_features)\n",
                "X_features, y = create_target(df_clean)\n",
                "\n",
                "print(f\"Data shape: {X_features.shape}\")\n",
                "print(f\"Target distribution: {y.value_counts()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Build Feature Matrix and Split Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build preprocessed features\n",
                "X, y, preprocessor = build_feature_matrix(data_path)\n",
                "\n",
                "# Split (use same indices for both raw and processed)\n",
                "X_train, X_test, y_train, y_test = train_test_split_stratified(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Get corresponding raw features for test set\n",
                "test_indices = X_test.index\n",
                "X_features_test = X_features.loc[test_indices]\n",
                "\n",
                "print(f\"Test set size: {X_test.shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Train Isolation Forest and Get Scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train model\n",
                "print(\"Training Isolation Forest...\")\n",
                "iso_forest = fit_isolation_forest(X_train.values, contamination=0.1, random_state=42)\n",
                "\n",
                "# Get anomaly scores\n",
                "if_scores_test = score_isolation_forest(iso_forest, X_test.values)\n",
                "print(\"\u2713 Model trained and scores computed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Compute Ontology Penalties"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply ontology rules to raw test features\n",
                "print(\"Applying ontology rules...\")\n",
                "ontology_penalties = apply_ontology_rules(X_features_test)\n",
                "\n",
                "print(f\"\\nOntology penalty distribution:\")\n",
                "print(ontology_penalties.value_counts().sort_index())\n",
                "print(f\"\\nMean penalty: {ontology_penalties.mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Combine ML Scores with Ontology Penalties"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine scores (alpha=0.7 for ML, beta=0.3 for ontology)\n",
                "combined_scores = combine_scores(if_scores_test, ontology_penalties.values, alpha=0.7, beta=0.3)\n",
                "\n",
                "print(f\"Combined scores range: [{combined_scores.min():.4f}, {combined_scores.max():.4f}]\")\n",
                "print(f\"Mean combined score: {combined_scores.mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Evaluate Both Approaches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate ML-only\n",
                "metrics_ml_only = evaluate_anomaly_detector(y_test.values, if_scores_test, model_name=\"Isolation Forest (ML Only)\")\n",
                "\n",
                "# Evaluate combined\n",
                "metrics_combined = evaluate_anomaly_detector(y_test.values, combined_scores, model_name=\"IF + Ontology\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print comparison table\n",
                "print_comparison_table([metrics_ml_only, metrics_combined])\n",
                "\n",
                "# Calculate improvement\n",
                "roc_improvement = ((metrics_combined['roc_auc'] - metrics_ml_only['roc_auc']) / metrics_ml_only['roc_auc']) * 100\n",
                "pr_improvement = ((metrics_combined['pr_auc'] - metrics_ml_only['pr_auc']) / metrics_ml_only['pr_auc']) * 100\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"PERFORMANCE IMPROVEMENT\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"ROC-AUC improvement: {roc_improvement:+.2f}%\")\n",
                "print(f\"PR-AUC improvement:  {pr_improvement:+.2f}%\")\n",
                "print(f\"{'='*60}\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot comparison\n",
                "plot_roc_pr(y_test.values, {\n",
                "    'IF (ML Only)': if_scores_test,\n",
                "    'IF + Ontology': combined_scores\n",
                "})\n",
                "plt.savefig(results_dir / 'ontology_comparison_curves.png', dpi=150, bbox_inches='tight')\n",
                "print(f\"\u2713 Comparison plots saved to {results_dir / 'ontology_comparison_curves.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Save Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save comparison metrics\n",
                "save_metrics_summary([metrics_ml_only, metrics_combined], results_dir / 'ontology_comparison_metrics.csv')\n",
                "print(\"\u2713 Ontology evaluation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "1. **Ontology-Inspired Rules**: Applied 3 clinical rules to compute domain-knowledge penalties\n",
                "2. **Score Combination**: Combined normalized ML scores with ontology penalties (\u03b1=0.7, \u03b2=0.3)\n",
                "3. **Performance Comparison**: Evaluated \"ML Only\" vs \"ML + Ontology\" approaches\n",
                "\n",
                "**Key Takeaway**: The ontology layer adds clinical domain knowledge to ML predictions, potentially improving identification of high-risk readmission cases."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}