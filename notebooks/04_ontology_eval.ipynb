{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ontology-Aware Anomaly Detection\n",
                "\n",
                "This notebook demonstrates how **domain knowledge** can be incorporated into anomaly detection through an ontology-inspired rule layer.\n",
                "\n",
                "**Approach**:\n",
                "1. Train baseline models (IF, AE) on normal patients\n",
                "2. Apply clinical ontology rules to compute risk penalties\n",
                "3. Combine ML scores with ontology penalties using different weights (lambda sweep)\n",
                "4. Compare performance improvements\n",
                "5. Examine individual patient cases where ontology rules fire"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import sys\n",
                "\n",
                "CWD = Path.cwd().resolve()\n",
                "if CWD.name == \"notebooks\":\n",
                "    PROJECT_ROOT = CWD.parent\n",
                "else:\n",
                "    PROJECT_ROOT = CWD\n",
                "\n",
                "sys.path.insert(0, str(PROJECT_ROOT))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.config import GLOBAL_CONFIG\n",
                "from src.preprocessing import (\n",
                "    load_raw_data,\n",
                "    get_selected_features,\n",
                "    clean_data,\n",
                "    create_target,\n",
                "    build_feature_matrix,\n",
                "    train_test_split_stratified,\n",
                ")\n",
                "from src.models import IsolationForestDetector, AutoencoderDetector\n",
                "from src.ontology import apply_ontology_rules, combine_scores\n",
                "from src.evaluation import compute_classification_metrics, plot_roc_pr_curves\n",
                "\n",
                "# Create results directory\n",
                "results_dir = PROJECT_ROOT / 'results'\n",
                "results_dir.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preparation\n",
                "\n",
                "We need two representations of the data:\n",
                "1. **Clinical features** (raw) for ontology rules\n",
                "2. **Encoded features** (scaled + one-hot) for ML models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load raw data\n",
                "data_path = PROJECT_ROOT / 'data' / 'raw' / 'diabetic_data.csv'\n",
                "df_raw = load_raw_data(str(data_path))\n",
                "print(f\"Loaded {len(df_raw):,} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build clinical feature DataFrame for ontology rules\n",
                "selected_features = get_selected_features()\n",
                "df_clean = clean_data(df_raw, selected_features, tracker=None)\n",
                "X_clinical_full, y_full = create_target(df_clean)\n",
                "\n",
                "print(f\"Clinical features shape: {X_clinical_full.shape}\")\n",
                "print(f\"Clinical features: {list(X_clinical_full.columns)[:10]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build encoded feature matrix for ML models\n",
                "X_encoded, y_encoded, preprocessor = build_feature_matrix(df_raw)\n",
                "\n",
                "print(f\"Encoded features shape: {X_encoded.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split (same split for both representations)\n",
                "cfg = GLOBAL_CONFIG\n",
                "\n",
                "X_clin_train, X_clin_test, y_clin_train, y_clin_test = train_test_split_stratified(\n",
                "    X_clinical_full,\n",
                "    y_full,\n",
                "    test_size=cfg.data.test_size,\n",
                "    random_state=cfg.data.random_seeds[0],\n",
                ")\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split_stratified(\n",
                "    X_encoded,\n",
                "    y_encoded,\n",
                "    test_size=cfg.data.test_size,\n",
                "    random_state=cfg.data.random_seeds[0],\n",
                ")\n",
                "\n",
                "print(f\"Test set size: {X_test.shape[0]:,} samples\")\n",
                "print(f\"Test positive rate: {y_test.mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Baseline Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Isolation Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train IF on normal samples only\n",
                "normal_mask = (y_train == 0)\n",
                "X_train_normal = X_train[normal_mask]\n",
                "\n",
                "print(f\"Training Isolation Forest on {X_train_normal.shape[0]:,} normal samples...\")\n",
                "\n",
                "if_detector = IsolationForestDetector(\n",
                "    n_estimators=cfg.isolation_forest.n_estimators,\n",
                "    contamination=float(y_train.mean()),\n",
                "    random_state=cfg.isolation_forest.random_state,\n",
                ")\n",
                "if_detector.fit(X_train_normal)\n",
                "\n",
                "if_scores_test = if_detector.predict_scores(X_test)\n",
                "print(f\"IF scores computed (range: [{if_scores_test.min():.4f}, {if_scores_test.max():.4f}])\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate baseline IF\n",
                "if_metrics = compute_classification_metrics(y_test, if_scores_test, model_name=\"IsolationForest\")\n",
                "\n",
                "print(\"\\nIsolation Forest (baseline)\")\n",
                "print(\"=\"*50)\n",
                "print(f\"ROC-AUC: {if_metrics['roc_auc']:.4f}\")\n",
                "print(f\"PR-AUC:  {if_metrics['pr_auc']:.4f}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Autoencoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train AE on normal samples only\n",
                "print(f\"Training Autoencoder on {X_train_normal.shape[0]:,} normal samples...\")\n",
                "\n",
                "ae_detector = AutoencoderDetector(\n",
                "    input_dim=X_train.shape[1],\n",
                "    hidden_dims=list(cfg.autoencoder.hidden_dims),\n",
                "    epochs=cfg.autoencoder.epochs,\n",
                "    batch_size=cfg.autoencoder.batch_size,\n",
                "    learning_rate=cfg.autoencoder.learning_rate,\n",
                ")\n",
                "ae_detector.fit(X_train_normal)\n",
                "\n",
                "ae_scores_test = ae_detector.predict_scores(X_test)\n",
                "print(f\"AE scores computed (range: [{ae_scores_test.min():.6f}, {ae_scores_test.max():.6f}])\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate baseline AE\n",
                "ae_metrics = compute_classification_metrics(y_test, ae_scores_test, model_name=\"Autoencoder\")\n",
                "\n",
                "print(\"\\nAutoencoder (baseline)\")\n",
                "print(\"=\"*50)\n",
                "print(f\"ROC-AUC: {ae_metrics['roc_auc']:.4f}\")\n",
                "print(f\"PR-AUC:  {ae_metrics['pr_auc']:.4f}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Apply Ontology Rules\n",
                "\n",
                "Clinical ontology rules identify high-risk patterns based on domain knowledge:\n",
                "- **Frequent inpatient admissions**: Patients with many prior inpatient visits\n",
                "- **Poor glycemic control without med changes**: High HbA1c/glucose but no medication adjustments\n",
                "- **Emergency visit without follow-up**: Emergency admissions without inpatient care"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply ontology rules to test set clinical features\n",
                "ontology_penalties_test, rule_stats = apply_ontology_rules(\n",
                "    X_clin_test,\n",
                "    y_clin_test.to_numpy(),\n",
                ")\n",
                "\n",
                "print(f\"Ontology penalties computed for {len(ontology_penalties_test):,} test samples\")\n",
                "print(f\"\\nPenalty distribution:\")\n",
                "print(pd.Series(ontology_penalties_test).value_counts().sort_index())\n",
                "print(f\"\\nMean penalty: {ontology_penalties_test.mean():.4f}\")\n",
                "print(f\"Max penalty: {ontology_penalties_test.max():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show rule statistics\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"ONTOLOGY RULE STATISTICS (TEST SET)\")\n",
                "print(\"=\"*70)\n",
                "print(f\"{'Rule Name':<40} {'Fired':>10} {'Fired & y=1':>12} {'Precision':>12}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "for rule_name, stats in rule_stats.items():\n",
                "    fired = stats.get('fired', 0)\n",
                "    fired_pos = stats.get('fired_positive', 0)\n",
                "    precision = (fired_pos / fired) if fired > 0 else 0.0\n",
                "    print(f\"{rule_name:<40} {fired:>10} {fired_pos:>12} {precision:>12.3f}\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lambda Sweep: Optimal Weight for Ontology\n",
                "\n",
                "We combine ML scores with ontology penalties using:\n",
                "```\n",
                "combined_score = (1-位) * ML_score + 位 * ontology_penalty\n",
                "```\n",
                "\n",
                "where 位 controls the weight given to domain knowledge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lambda sweep parameters\n",
                "lambda_values = [0.0, 0.1, 0.3, 0.5]\n",
                "\n",
                "# Storage for results\n",
                "if_combo_results = []\n",
                "ae_combo_results = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### IF + Ontology Lambda Sweep"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Isolation Forest + Ontology Lambda Sweep\")\n",
                "print(\"=\"*50)\n",
                "print(f\"{'Lambda':<10} {'ROC-AUC':<12} {'PR-AUC':<12}\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "for lam in lambda_values:\n",
                "    alpha = 1.0 - lam\n",
                "    beta = lam\n",
                "    \n",
                "    scores_lam = combine_scores(\n",
                "        if_scores_test,\n",
                "        ontology_penalties_test,\n",
                "        alpha=alpha,\n",
                "        beta=beta,\n",
                "        normalize_ml=True,\n",
                "    )\n",
                "    \n",
                "    metrics_lam = compute_classification_metrics(\n",
                "        y_test,\n",
                "        scores_lam,\n",
                "        model_name=f\"IF+Ontology(lambda={lam:.2f})\",\n",
                "    )\n",
                "    \n",
                "    if_combo_results.append((lam, scores_lam, metrics_lam))\n",
                "    \n",
                "    print(f\"{lam:<10.2f} {metrics_lam['roc_auc']:<12.4f} {metrics_lam['pr_auc']:<12.4f}\")\n",
                "\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select best lambda for IF based on PR-AUC, then ROC-AUC\n",
                "best_idx_if = max(\n",
                "    range(len(if_combo_results)),\n",
                "    key=lambda i: (if_combo_results[i][2]['pr_auc'], if_combo_results[i][2]['roc_auc'])\n",
                ")\n",
                "best_lambda_if, best_scores_if, best_metrics_if = if_combo_results[best_idx_if]\n",
                "\n",
                "print(f\"\\nBest lambda for IF+Ontology: {best_lambda_if:.2f}\")\n",
                "print(f\"  ROC-AUC: {best_metrics_if['roc_auc']:.4f}\")\n",
                "print(f\"  PR-AUC:  {best_metrics_if['pr_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### AE + Ontology Lambda Sweep"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nAutoencoder + Ontology Lambda Sweep\")\n",
                "print(\"=\"*50)\n",
                "print(f\"{'Lambda':<10} {'ROC-AUC':<12} {'PR-AUC':<12}\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "for lam in lambda_values:\n",
                "    alpha = 1.0 - lam\n",
                "    beta = lam\n",
                "    \n",
                "    scores_lam = combine_scores(\n",
                "        ae_scores_test,\n",
                "        ontology_penalties_test,\n",
                "        alpha=alpha,\n",
                "        beta=beta,\n",
                "        normalize_ml=True,\n",
                "    )\n",
                "    \n",
                "    metrics_lam = compute_classification_metrics(\n",
                "        y_test,\n",
                "        scores_lam,\n",
                "        model_name=f\"AE+Ontology(lambda={lam:.2f})\",\n",
                "    )\n",
                "    \n",
                "    ae_combo_results.append((lam, scores_lam, metrics_lam))\n",
                "    \n",
                "    print(f\"{lam:<10.2f} {metrics_lam['roc_auc']:<12.4f} {metrics_lam['pr_auc']:<12.4f}\")\n",
                "\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select best lambda for AE based on PR-AUC, then ROC-AUC\n",
                "best_idx_ae = max(\n",
                "    range(len(ae_combo_results)),\n",
                "    key=lambda i: (ae_combo_results[i][2]['pr_auc'], ae_combo_results[i][2]['roc_auc'])\n",
                ")\n",
                "best_lambda_ae, best_scores_ae, best_metrics_ae = ae_combo_results[best_idx_ae]\n",
                "\n",
                "print(f\"\\nBest lambda for AE+Ontology: {best_lambda_ae:.2f}\")\n",
                "print(f\"  ROC-AUC: {best_metrics_ae['roc_auc']:.4f}\")\n",
                "print(f\"  PR-AUC:  {best_metrics_ae['pr_auc']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_df = pd.DataFrame([\n",
                "    {\n",
                "        'Model': 'Isolation Forest',\n",
                "        'ROC-AUC': if_metrics['roc_auc'],\n",
                "        'PR-AUC': if_metrics['pr_auc'],\n",
                "    },\n",
                "    {\n",
                "        'Model': f'IF + Ontology (lambda={best_lambda_if:.2f})',\n",
                "        'ROC-AUC': best_metrics_if['roc_auc'],\n",
                "        'PR-AUC': best_metrics_if['pr_auc'],\n",
                "    },\n",
                "    {\n",
                "        'Model': 'Autoencoder',\n",
                "        'ROC-AUC': ae_metrics['roc_auc'],\n",
                "        'PR-AUC': ae_metrics['pr_auc'],\n",
                "    },\n",
                "    {\n",
                "        'Model': f'AE + Ontology (lambda={best_lambda_ae:.2f})',\n",
                "        'ROC-AUC': best_metrics_ae['roc_auc'],\n",
                "        'PR-AUC': best_metrics_ae['pr_auc'],\n",
                "    },\n",
                "])\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL COMPARISON: BASELINE vs ONTOLOGY-ENHANCED\")\n",
                "print(\"=\"*70)\n",
                "print(comparison_df.to_string(index=False))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute improvements\n",
                "if_roc_improvement = ((best_metrics_if['roc_auc'] - if_metrics['roc_auc']) / if_metrics['roc_auc']) * 100\n",
                "if_pr_improvement = ((best_metrics_if['pr_auc'] - if_metrics['pr_auc']) / if_metrics['pr_auc']) * 100\n",
                "\n",
                "ae_roc_improvement = ((best_metrics_ae['roc_auc'] - ae_metrics['roc_auc']) / ae_metrics['roc_auc']) * 100\n",
                "ae_pr_improvement = ((best_metrics_ae['pr_auc'] - ae_metrics['pr_auc']) / ae_metrics['pr_auc']) * 100\n",
                "\n",
                "print(\"\\nPerformance Improvements:\")\n",
                "print(\"-\"*50)\n",
                "print(f\"IF + Ontology:\")\n",
                "print(f\"  ROC-AUC: {if_roc_improvement:+.2f}%\")\n",
                "print(f\"  PR-AUC:  {if_pr_improvement:+.2f}%\")\n",
                "print(f\"\\nAE + Ontology:\")\n",
                "print(f\"  ROC-AUC: {ae_roc_improvement:+.2f}%\")\n",
                "print(f\"  PR-AUC:  {ae_pr_improvement:+.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization: ROC and PR Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot IF baseline vs IF+Ontology\n",
                "from src.evaluation import plot_evaluation_curves\n",
                "\n",
                "plot_evaluation_curves(\n",
                "    y_test,\n",
                "    {\n",
                "        'IF (baseline)': if_scores_test,\n",
                "        f'IF + Ontology (lambda={best_lambda_if:.2f})': best_scores_if,\n",
                "    }\n",
                ")\n",
                "plt.savefig(results_dir / 'nb_ontology_if_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved: {results_dir / 'nb_ontology_if_comparison.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot AE baseline vs AE+Ontology\n",
                "plot_evaluation_curves(\n",
                "    y_test,\n",
                "    {\n",
                "        'AE (baseline)': ae_scores_test,\n",
                "        f'AE + Ontology (lambda={best_lambda_ae:.2f})': best_scores_ae,\n",
                "    }\n",
                ")\n",
                "plt.savefig(results_dir / 'nb_ontology_ae_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved: {results_dir / 'nb_ontology_ae_comparison.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Case Studies: Individual Patients\n",
                "\n",
                "Let's examine specific patients where ontology rules fired to understand how domain knowledge affects risk scores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find patients where at least one rule fired\n",
                "patients_with_penalties = np.where(ontology_penalties_test > 0)[0]\n",
                "\n",
                "print(f\"Found {len(patients_with_penalties):,} patients with ontology penalties\")\n",
                "\n",
                "# Select 3 interesting cases: different penalty levels\n",
                "penalty_levels = ontology_penalties_test[patients_with_penalties]\n",
                "sorted_indices = np.argsort(penalty_levels)[::-1]  # Highest penalties first\n",
                "\n",
                "# Select: highest penalty, median penalty, and one readmitted patient\n",
                "case_indices = []\n",
                "if len(sorted_indices) > 0:\n",
                "    case_indices.append(patients_with_penalties[sorted_indices[0]])  # Highest penalty\n",
                "if len(sorted_indices) > len(sorted_indices)//2:\n",
                "    case_indices.append(patients_with_penalties[sorted_indices[len(sorted_indices)//2]])  # Median\n",
                "\n",
                "# Find a readmitted patient with penalties\n",
                "readmitted_with_penalty = np.where((ontology_penalties_test > 0) & (y_test.values == 1))[0]\n",
                "if len(readmitted_with_penalty) > 0:\n",
                "    case_indices.append(readmitted_with_penalty[0])\n",
                "\n",
                "print(f\"\\nSelected {len(case_indices)} cases for detailed analysis\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display case studies\n",
                "key_clinical_features = [\n",
                "    'time_in_hospital', 'number_inpatient', 'number_emergency', \n",
                "    'number_outpatient', 'num_medications', 'num_lab_procedures'\n",
                "]\n",
                "\n",
                "for i, case_idx in enumerate(case_indices, 1):\n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(f\"CASE STUDY {i}: Patient Index {case_idx}\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    # Get patient data\n",
                "    patient_clinical = X_clin_test.iloc[case_idx]\n",
                "    true_label = y_test.iloc[case_idx]\n",
                "    penalty = ontology_penalties_test[case_idx]\n",
                "    \n",
                "    # Get scores\n",
                "    if_score_before = if_scores_test[case_idx]\n",
                "    if_score_after = best_scores_if[case_idx]\n",
                "    ae_score_before = ae_scores_test[case_idx]\n",
                "    ae_score_after = best_scores_ae[case_idx]\n",
                "    \n",
                "    print(f\"\\nTrue Readmission Status: {'Readmitted <30 days' if true_label == 1 else 'Not readmitted <30'}\")\n",
                "    print(f\"Ontology Penalty: {penalty:.4f}\")\n",
                "    \n",
                "    # Show clinical features\n",
                "    print(f\"\\nKey Clinical Features:\")\n",
                "    available_features = [f for f in key_clinical_features if f in patient_clinical.index]\n",
                "    for feat in available_features:\n",
                "        print(f\"  {feat}: {patient_clinical[feat]}\")\n",
                "    \n",
                "    # Show score changes\n",
                "    print(f\"\\nIsolation Forest Scores:\")\n",
                "    print(f\"  Before Ontology: {if_score_before:.4f}\")\n",
                "    print(f\"  After Ontology:  {if_score_after:.4f} (change: {if_score_after - if_score_before:+.4f})\")\n",
                "    \n",
                "    print(f\"\\nAutoencoder Scores:\")\n",
                "    print(f\"  Before Ontology: {ae_score_before:.6f}\")\n",
                "    print(f\"  After Ontology:  {ae_score_after:.6f} (change: {ae_score_after - ae_score_before:+.6f})\")\n",
                "    \n",
                "    # Identify which rules likely fired (simplified heuristic)\n",
                "    print(f\"\\nLikely Rules Fired:\")\n",
                "    if patient_clinical.get('number_inpatient', 0) >= 2:\n",
                "        print(f\"  - Frequent inpatient admissions (number_inpatient = {patient_clinical.get('number_inpatient', 0)})\")\n",
                "    if patient_clinical.get('number_emergency', 0) >= 1 and patient_clinical.get('number_inpatient', 0) == 0:\n",
                "        print(f\"  - Emergency visit without inpatient follow-up\")\n",
                "    # Note: HbA1c/glucose rules require more complex logic\n",
                "    \n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Key Findings**:\n",
                "\n",
                "1. **Ontology rules** successfully identify clinically suspicious patterns:\n",
                "   - Frequent inpatient admissions signal chronic healthcare needs\n",
                "   - Emergency visits without follow-up care indicate care coordination gaps\n",
                "   - Poor glycemic control without medication changes suggests inadequate management\n",
                "\n",
                "2. **Lambda sweep** determines optimal weighting between ML scores and domain knowledge:\n",
                "   - Small 位 values (0.1-0.3) typically work well, preserving ML signals while adding clinical context\n",
                "   - Performance on PR-AUC often shows more improvement than ROC-AUC (important for imbalanced data)\n",
                "\n",
                "3. **Case studies** reveal how ontology adjusts risk scores:\n",
                "   - Patients with multiple risk factors get higher combined scores\n",
                "   - The ontology layer acts as a \"clinical filter\" that amplifies risk for patients meeting known danger patterns\n",
                "\n",
                "4. **Practical value**: Ontology rules provide **interpretability** - unlike pure ML scores, we can explain *why* a patient's risk increased based on specific clinical criteria."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}