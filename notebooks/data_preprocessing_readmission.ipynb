{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Diabetes Hospital Readmission Data Preprocessing\n",
                "\n",
                "Loading and preprocessing the UCI Diabetes 130-US Hospitals dataset for readmission prediction."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Libraries and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Set display options\n",
                "pd.set_option('display.max_columns', 50)\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv('../data/diabetic_data.csv')\n",
                "\n",
                "print(\"Dataset loaded successfully!\")\n",
                "print(f\"Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Basic Data Information"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display basic info\n",
                "print(\"=\" * 50)\n",
                "print(\"Dataset Shape:\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Rows: {df.shape[0]:,}\")\n",
                "print(f\"Columns: {df.shape[1]}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Data Types:\")\n",
                "print(\"=\"*50)\n",
                "print(df.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed info\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values (including '?' which is common in this dataset)\n",
                "print(\"Missing Values Analysis:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Count '?' as missing\n",
                "missing_counts = (df == '?').sum()\n",
                "missing_pct = (missing_counts / len(df)) * 100\n",
                "\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_counts,\n",
                "    'Percentage': missing_pct\n",
                "})\n",
                "\n",
                "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Percentage', ascending=False)\n",
                "print(missing_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Target Variable Analysis: `readmitted`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze readmitted column\n",
                "print(\"Readmitted Value Counts:\")\n",
                "print(\"=\"*50)\n",
                "print(df['readmitted'].value_counts())\n",
                "print(\"\\nPercentages:\")\n",
                "print(df['readmitted'].value_counts(normalize=True) * 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize readmitted distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Count plot\n",
                "readmit_counts = df['readmitted'].value_counts()\n",
                "axes[0].bar(readmit_counts.index, readmit_counts.values, \n",
                "            color=['green', 'orange', 'red'], alpha=0.7)\n",
                "axes[0].set_xlabel('Readmitted Category')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title('Distribution of Readmission Status')\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add value labels on bars\n",
                "for i, (idx, val) in enumerate(readmit_counts.items()):\n",
                "    axes[0].text(i, val, f'{val:,}', ha='center', va='bottom')\n",
                "\n",
                "# Percentage plot\n",
                "readmit_pct = df['readmitted'].value_counts(normalize=True) * 100\n",
                "axes[1].bar(readmit_pct.index, readmit_pct.values, \n",
                "            color=['green', 'orange', 'red'], alpha=0.7)\n",
                "axes[1].set_xlabel('Readmitted Category')\n",
                "axes[1].set_ylabel('Percentage (%)')\n",
                "axes[1].set_title('Distribution of Readmission Status (%)')\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add percentage labels\n",
                "for i, (idx, val) in enumerate(readmit_pct.items()):\n",
                "    axes[1].text(i, val, f'{val:.1f}%', ha='center', va='bottom')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the subset of useful features\n",
                "selected_features = [\n",
                "    # Demographics\n",
                "    'race', 'gender', 'age',\n",
                "    \n",
                "    # Hospital stay metrics\n",
                "    'time_in_hospital',\n",
                "    \n",
                "    # Procedure counts\n",
                "    'num_lab_procedures',\n",
                "    'num_procedures',\n",
                "    'num_medications',\n",
                "    \n",
                "    # Outpatient/Emergency visits\n",
                "    'number_outpatient',\n",
                "    'number_inpatient',\n",
                "    'number_emergency',\n",
                "    \n",
                "    # Lab results\n",
                "    'A1Cresult',\n",
                "    'max_glu_serum',\n",
                "    \n",
                "    # Medication changes\n",
                "    'change',\n",
                "    'diabetesMed'\n",
                "]\n",
                "\n",
                "# Verify all features exist\n",
                "missing_features = [f for f in selected_features if f not in df.columns]\n",
                "if missing_features:\n",
                "    print(f\"Warning: Missing features: {missing_features}\")\n",
                "else:\n",
                "    print(\"✓ All selected features are present in the dataset\")\n",
                "\n",
                "print(f\"\\nSelected {len(selected_features)} features\")\n",
                "print(selected_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create working dataframe with selected features + target\n",
                "df_subset = df[selected_features + ['readmitted']].copy()\n",
                "\n",
                "print(f\"Subset shape: {df_subset.shape}\")\n",
                "df_subset.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace '?' with NaN\n",
                "df_clean = df_subset.replace('?', np.nan)\n",
                "\n",
                "print(\"Replaced '?' with NaN\")\n",
                "print(\"\\nMissing values per column:\")\n",
                "missing_info = df_clean.isnull().sum()\n",
                "missing_info = missing_info[missing_info > 0].sort_values(ascending=False)\n",
                "print(missing_info)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop rows with missing values in critical features\n",
                "# (Alternative: you could impute, but for simplicity we'll drop)\n",
                "print(f\"Rows before cleaning: {len(df_clean):,}\")\n",
                "\n",
                "df_clean = df_clean.dropna()\n",
                "\n",
                "print(f\"Rows after cleaning: {len(df_clean):,}\")\n",
                "print(f\"Rows dropped: {len(df_subset) - len(df_clean):,} ({(len(df_subset) - len(df_clean))/len(df_subset)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify no missing values remain\n",
                "print(\"Remaining missing values:\")\n",
                "print(df_clean.isnull().sum().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Create Binary Target Variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary target: y = 1 if readmitted < 30 days, else 0\n",
                "y = (df_clean['readmitted'] == '<30').astype(int)\n",
                "\n",
                "print(\"Binary Target Distribution:\")\n",
                "print(\"=\"*50)\n",
                "print(f\"y = 1 (readmitted < 30 days): {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
                "print(f\"y = 0 (not readmitted < 30): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
                "print(f\"\\nClass imbalance ratio: {(y == 0).sum() / (y == 1).sum():.2f}:1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize binary target\n",
                "plt.figure(figsize=(8, 5))\n",
                "y_counts = y.value_counts()\n",
                "plt.bar(['Not Readmitted <30', 'Readmitted <30'], \n",
                "        [y_counts[0], y_counts[1]], \n",
                "        color=['green', 'red'], alpha=0.7)\n",
                "plt.ylabel('Count')\n",
                "plt.title('Binary Classification Target Distribution')\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add counts on bars\n",
                "for i, val in enumerate([y_counts[0], y_counts[1]]):\n",
                "    plt.text(i, val, f'{val:,}\\n({val/len(y)*100:.1f}%)', \n",
                "             ha='center', va='bottom')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Encoding and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features from target\n",
                "X = df_clean.drop('readmitted', axis=1)\n",
                "\n",
                "print(f\"Feature matrix shape: {X.shape}\")\n",
                "print(f\"Target vector shape: {y.shape}\")\n",
                "\n",
                "# Identify categorical and numerical columns\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                "\n",
                "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
                "print(f\"\\nNumerical features ({len(numerical_cols)}): {numerical_cols}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create preprocessing pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Define transformers\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_cols),\n",
                "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
                "         categorical_cols)\n",
                "    ],\n",
                "    remainder='passthrough'\n",
                ")\n",
                "\n",
                "print(\"Preprocessing pipeline created:\")\n",
                "print(preprocessor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit and transform the feature matrix\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "\n",
                "print(f\"Original feature matrix shape: {X.shape}\")\n",
                "print(f\"Transformed feature matrix shape: {X_transformed.shape}\")\n",
                "print(f\"\\nFeatures expanded due to one-hot encoding: {X_transformed.shape[1] - X.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature names after transformation\n",
                "feature_names = []\n",
                "\n",
                "# Numerical features (scaled)\n",
                "feature_names.extend(numerical_cols)\n",
                "\n",
                "# Categorical features (one-hot encoded)\n",
                "if hasattr(preprocessor.named_transformers_['cat'], 'get_feature_names_out'):\n",
                "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
                "    feature_names.extend(cat_features)\n",
                "\n",
                "print(f\"Total features after encoding: {len(feature_names)}\")\n",
                "print(f\"\\nFirst 20 feature names: {feature_names[:20]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a clean DataFrame with transformed features\n",
                "X_final = pd.DataFrame(\n",
                "    X_transformed, \n",
                "    columns=feature_names,\n",
                "    index=X.index\n",
                ")\n",
                "\n",
                "print(\"Final feature matrix:\")\n",
                "print(X_final.head())\n",
                "print(f\"\\nShape: {X_final.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Final Dataset Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"FINAL PREPROCESSED DATASET SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nSamples: {X_final.shape[0]:,}\")\n",
                "print(f\"Features: {X_final.shape[1]:,}\")\n",
                "print(f\"\\nTarget distribution:\")\n",
                "print(f\"  Class 0 (not readmitted <30): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
                "print(f\"  Class 1 (readmitted <30):     {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
                "print(f\"\\nFeature types:\")\n",
                "print(f\"  Original numerical: {len(numerical_cols)}\")\n",
                "print(f\"  Original categorical: {len(categorical_cols)}\")\n",
                "print(f\"  After one-hot encoding: {X_final.shape[1]}\")\n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display sample statistics\n",
                "print(\"Sample statistics (first 5 numerical features):\")\n",
                "X_final.iloc[:, :5].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Preprocessed Data (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optional: Save preprocessed data for later use\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "\n",
                "# Create output directory\n",
                "output_dir = Path('../data/processed')\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Save features and target\n",
                "X_final.to_csv(output_dir / 'X_features.csv', index=False)\n",
                "y.to_csv(output_dir / 'y_target.csv', index=False, header=['target'])\n",
                "\n",
                "# Save preprocessor for future use\n",
                "with open(output_dir / 'preprocessor.pkl', 'wb') as f:\n",
                "    pickle.dump(preprocessor, f)\n",
                "\n",
                "print(\"✓ Preprocessed data saved to data/processed/\")\n",
                "print(f\"  - X_features.csv ({X_final.shape})\")\n",
                "print(f\"  - y_target.csv ({y.shape})\")\n",
                "print(f\"  - preprocessor.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ready for Modeling!\n",
                "\n",
                "You now have:\n",
                "- **`X_final`**: Clean feature matrix (numerical features scaled, categorical one-hot encoded)\n",
                "- **`y`**: Binary target (1 = readmitted within 30 days, 0 = otherwise)\n",
                "- **`preprocessor`**: Fitted sklearn pipeline for transforming new data\n",
                "\n",
                "Next steps:\n",
                "1. Train/test split\n",
                "2. Model training (e.g., Logistic Regression, Random Forest, XGBoost)\n",
                "3. Evaluation (ROC-AUC, Precision-Recall, etc.)\n",
                "4. Feature importance analysis"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}