{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - Data Preprocessing\n",
                "\n",
                "Data cleaning, feature engineering, and preparation for anomaly detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from src.data_loader import load_raw_data, save_processed_data\n",
                "from src.preprocessing import (\n",
                "    handle_missing_values,\n",
                "    encode_categorical_features,\n",
                "    extract_features,\n",
                "    scale_features,\n",
                "    preprocess_data\n",
                ")\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Raw Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_raw = load_raw_data()\n",
                "print(f\"Raw data shape: {df_raw.shape}\")\n",
                "df_raw.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check missing values\n",
                "missing_before = df_raw.isnull().sum().sum()\n",
                "print(f\"Total missing values: {missing_before}\")\n",
                "\n",
                "# Handle missing values (choose strategy: 'drop', 'mean', 'median', 'mode')\n",
                "df_clean = handle_missing_values(df_raw, strategy='drop')\n",
                "\n",
                "missing_after = df_clean.isnull().sum().sum()\n",
                "print(f\"\\nAfter handling:\")\n",
                "print(f\"  Total missing values: {missing_after}\")\n",
                "print(f\"  Records remaining: {len(df_clean)} ({len(df_clean)/len(df_raw)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and engineer features\n",
                "df_features = extract_features(df_clean)\n",
                "\n",
                "print(f\"Features added:\")\n",
                "new_cols = set(df_features.columns) - set(df_clean.columns)\n",
                "for col in new_cols:\n",
                "    print(f\"  - {col}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize engineered features\n",
                "if 'medication_count' in df_features.columns:\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    \n",
                "    plt.subplot(1, 2, 1)\n",
                "    df_features['medication_count'].hist(bins=20, edgecolor='black')\n",
                "    plt.title('Distribution of Medication Count')\n",
                "    plt.xlabel('Number of Medications')\n",
                "    plt.ylabel('Frequency')\n",
                "    \n",
                "    if 'diagnosis_count' in df_features.columns:\n",
                "        plt.subplot(1, 2, 2)\n",
                "        df_features['diagnosis_count'].hist(bins=20, edgecolor='black')\n",
                "        plt.title('Distribution of Diagnosis Count')\n",
                "        plt.xlabel('Number of Diagnoses')\n",
                "        plt.ylabel('Frequency')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encode Categorical Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode categorical variables\n",
                "df_encoded, encoders = encode_categorical_features(df_features)\n",
                "\n",
                "print(f\"Encoded {len(encoders)} categorical features:\")\n",
                "for col in list(encoders.keys())[:5]:  # Show first 5\n",
                "    print(f\"  - {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale numerical features\n",
                "df_scaled, scaler = scale_features(df_encoded)\n",
                "\n",
                "print(\"Features scaled using StandardScaler\")\n",
                "print(f\"Final preprocessed data shape: {df_scaled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Complete Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run full preprocessing pipeline\n",
                "df_processed, artifacts = preprocess_data(df_raw, handle_missing='drop', scale=True)\n",
                "\n",
                "print(\"\\n=== Preprocessing Complete ===\")\n",
                "print(f\"Original data: {df_raw.shape}\")\n",
                "print(f\"Processed data: {df_processed.shape}\")\n",
                "print(f\"\\nArtifacts saved: {list(artifacts.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to data/processed/\n",
                "save_processed_data(df_processed, filename='processed_data.csv')\n",
                "\n",
                "# Also save artifacts for later use\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "\n",
                "artifacts_path = Path('..') / 'data' / 'processed' / 'preprocessing_artifacts.pkl'\n",
                "with open(artifacts_path, 'wb') as f:\n",
                "    pickle.dump(artifacts, f)\n",
                "    \n",
                "print(f\"Saved preprocessing artifacts to: {artifacts_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final data summary\n",
                "df_processed.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Proceed to `03_anomaly_detection.ipynb` to apply anomaly detection algorithms."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}